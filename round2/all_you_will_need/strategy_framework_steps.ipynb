{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Appropriate Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import random\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import talib as ta\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, hp, partial, tpe, Trials\n",
    "\n",
    "#silence warnings output...\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#following for the ML part...setting a seed meant that multiple executions will yield the same result.\n",
    "random_state = 42\n",
    "random.seed(random_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and Load CSV (Modify Path Here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path1 = '''/kaggle/input/hku-qids-2023-quantitative-investment-competition/qids_package/'''\n",
    "data_path2 = '''/kaggle/input/hku-qids-2023-quantitative-investment-competition/'''\n",
    "fn_fund = '''first_round_test_fundamental_data.csv'''\n",
    "fn_market = '''first_round_test_market_data.csv'''\n",
    "fn_train_fund = '''first_round_train_fundamental_data.csv'''\n",
    "fn_train_market = '''first_round_train_market_data.csv'''\n",
    "fn_train_returns = '''first_round_train_return_data.csv'''\n",
    "\n",
    "df_fund = pd.read_csv(fn_fund)\n",
    "df_market = pd.read_csv(fn_market)\n",
    "df_train_fund = pd.read_csv(fn_train_fund)\n",
    "df_train_market = pd.read_csv(fn_train_market)\n",
    "df_train_returns = pd.read_csv(fn_train_returns)\n",
    "\n",
    "df_list = [df_fund, df_market, df_train_fund, df_train_market, df_train_returns]\n",
    "df_names = ['df_fund', 'df_market', 'df_train_fund', 'df_train_market', 'df_train_returns']\n",
    "for df, df_name in zip(df_list, df_names):\n",
    "    print(f\"Columns of {df_name} are: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Create columns containing data column of \"date_time\"\n",
    "def interval_split(dt):\n",
    "    #for date_time column of \"sXXdXXpXX\"\n",
    "    f2, f3 = dt.find(\"d\"), dt.find(\"p\")\n",
    "    return [int(dt[1:f2]), int(dt[f2+1:f3]), int(dt[f3+1:])]\n",
    "def date_split(dt):\n",
    "    #for date_time column of \"sXXdXX\"\n",
    "    f2 = dt.find(\"d\")\n",
    "    return [int(dt[1:f2]), int(dt[f2+1:])]\n",
    "def add_interval(df):\n",
    "    df_interval_data = np.vstack(df.date_time.apply(lambda x: interval_split(x)))\n",
    "    df[[\"asset\", \"day\", \"interval\"]] = df_interval_data\n",
    "    return df\n",
    "def add_date(df):\n",
    "    df_date_data = np.vstack(df.date_time.apply(lambda x: date_split(x)))\n",
    "    df[[\"asset\", \"day\"]] = df_date_data\n",
    "    return df\n",
    "print(\"Ran\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_period(df):\n",
    "    #no intervals\n",
    "    df_period = df[['asset', 'day']]\n",
    "    for i in range(2, 15):\n",
    "        df_period[f\"period{i}\"] = df[\"day\"] // i\n",
    "    return df_period\n",
    "def add_remainder(df):\n",
    "    #no intervals\n",
    "    df_remainder = df[['asset', 'day']]\n",
    "    for i in range(2, 15):\n",
    "        df_remainder[f\"remainder{i}\"] = df[\"day\"] % i\n",
    "    return df_remainder\n",
    "\n",
    "def ctc_returns(df_market_a):\n",
    "    #no intervals\n",
    "    df_ctc = df_market_a[['asset', 'day']].reset_index(drop=True)\n",
    "    for days in [1, 5, 10, 20]:\n",
    "        df_ctc[f\"ctc{days}\"] = df_market_a['close'].pct_change(days).reset_index(drop=True)\n",
    "    return df_ctc\n",
    "\n",
    "\n",
    "def daily_volume_moving_ratio(df_market_a):\n",
    "    #contains intervals\n",
    "    #take df by asset type\n",
    "    daily_volume_series = df_market_a.groupby(df_market_a['day'])['volume'].sum()\n",
    "    df_movv = df_market_a[['asset', 'day']].drop_duplicates(subset=[\"asset\", \"day\"], keep='last').reset_index(drop=True)\n",
    "    df_movv[\"daily_volume_moving_ratio_5d\"] = daily_volume_series / daily_volume_series.rolling(5).mean()\n",
    "    df_movv[\"daily_volume_moving_ratio_10d\"] = daily_volume_series / daily_volume_series.rolling(10).mean()\n",
    "    df_movv[\"daily_volume_moving_ratio_20d\"] = daily_volume_series / daily_volume_series.rolling(20).mean()\n",
    "    return df_movv\n",
    "\n",
    "\n",
    "#Talib Features **\n",
    "def add_talib_features(df_market_a):\n",
    "    #This function take reference to the \"construct_talib_features\" function and tailor to our dataframe settings\n",
    "    #by creating asset, day, interval columns (prior), following code should be more understandable\n",
    "    close_p = df_market_a[df_market_a['interval'] == 50]['close'].reset_index(drop=True)\n",
    "    high_p = df_market_a.groupby(df_market_a['day'])['high'].max().reset_index(drop=True)\n",
    "    low_p = df_market_a.groupby(df_market_a['day'])['low'].min().reset_index(drop=True)\n",
    "\n",
    "    feature_df = df_market_a[['asset', 'day']].drop_duplicates(subset=[\"asset\", \"day\"], keep='last').reset_index(drop=True)\n",
    "    feature_df[\"SAR\"] = ta.SAR(high_p, low_p, acceleration = 0, maximum = 0)\n",
    "    feature_df[\"SAREXT\"] = ta.SAREXT(high_p, low_p, startvalue = 0, offsetonreverse = 0, accelerationinitlong = 0, accelerationlong = 0, accelerationmaxlong = 0, accelerationinitshort = 0, accelerationshort = 0, accelerationmaxshort = 0)\n",
    "    feature_df[\"RSI\"] = ta.RSI(close_p, timeperiod = 14) - 50\n",
    "    feature_df[\"HT_DCPERIOD\"] = ta.HT_DCPERIOD(close_p)\n",
    "    feature_df[\"HT_PHASOR_inphase\"], feature_df[\"HT_PHASOR_quadrature\"] = ta.HT_PHASOR(close_p)\n",
    "    feature_df[\"HT_SINE_sine\"], feature_df[\"HT_SINE_leadsine\"] = ta.HT_SINE(close_p)\n",
    "    return feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features in Consideration\n",
    "def merge_columns(market, fundamental, returns=pd.DataFrame([0])):\n",
    "    market = add_interval(market)\n",
    "    fundamental = add_date(fundamental)\n",
    "    df = pd.merge(market, fundamental, left_on=[\"asset\", \"day\"], right_on=[\"asset\", \"day\"])\n",
    "    if returns.any()[0]:    #check if returns is a non-empty dataframe\n",
    "        returns = add_date(returns)\n",
    "        df = pd.merge(df, returns, left_on=[\"asset\", \"day\"], right_on=[\"asset\", \"day\"])\n",
    "    # Note: This Step Will Mean that Metrics regarding\n",
    "    return df\n",
    "    # df = df.drop_duplicates(subset=[\"asset\", \"day\"], keep='last').reset_index(drop=True)\n",
    "\n",
    "def add_features(df):\n",
    "    #THIS will return columns with only one set of ['asset', 'day'] (no intervals)\n",
    "    df_close_only = df.drop_duplicates(subset=[\"asset\", \"day\"], keep='last').reset_index(drop=True)\n",
    "    df_features = df_close_only[['asset', 'day']]     #a df that contains the new feature to reduce loading time in each function call.\n",
    "\n",
    "    #features not using intervals data\n",
    "    df_features = pd.merge(df_features, add_period(df_close_only), left_on=[\"asset\", \"day\"], right_on=[\"asset\", \"day\"])\n",
    "    df_features = pd.merge(df_features, add_remainder(df_close_only), left_on=[\"asset\", \"day\"], right_on=[\"asset\", \"day\"])\n",
    "    df_features = pd.merge(df_close_only, df_features, left_on=[\"asset\", \"day\"], right_on=[\"asset\", \"day\"])\n",
    "\n",
    "    #ctc returns\n",
    "    ctc_features = pd.concat([ctc_returns(df_close_only[df_close_only['asset'] == i]) for i in range(54)]).reset_index(drop=True)\n",
    "    df_features = pd.merge(df_features, ctc_features, left_on=[\"asset\", \"day\"], right_on=[\"asset\", \"day\"])\n",
    "\n",
    "    #moving volume\n",
    "    movv_features = pd.concat([daily_volume_moving_ratio(df[df['asset'] == i]) for i in range(54)]).reset_index(drop=True)\n",
    "    df_features = pd.merge(df_features, movv_features, left_on=[\"asset\", \"day\"], right_on=[\"asset\", \"day\"])\n",
    "\n",
    "    #talib features: iterate over each asset together, then merge by [\"asset\", \"day\"]\n",
    "    talib_features = pd.concat([add_talib_features(df[df['asset'] == i]) for i in range(54)]).reset_index(drop=True)\n",
    "    df_features = pd.merge(df_features, talib_features, left_on=[\"asset\", \"day\"], right_on=[\"asset\", \"day\"])\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = merge_columns(df_market, df_fund, df_return)\n",
    "df = add_features(df_temp)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Insights on Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_to_check = 43\n",
    "df_temp = df[(df['asset'] == asset_to_check) & (df['day'] <= 1000)][['close', 'RSI']]\n",
    "ax = df_temp[['close']].plot(figsize=(30, 15), color='red')\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_ylabel(\"close price\")\n",
    "#ax2 is to construct a second y axis on the graph (ax object)\n",
    "ax2 = ax.twinx()\n",
    "df_temp[['RSI']].plot(ax=ax2, style=\"--\")\n",
    "ax2.set_ylabel(\"RSI\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it to the Model (Reusing Part1's XGBoost (replace with lightgbm later?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag(df, target_map):\n",
    "    for i in [700, 750, 800, 850, 900, 950]:   #needs to be larger than the 700 days in the future.\n",
    "        df[f'lag{i}'] = (df.index - i*54).map(target_map)       #since each asset row is distanted at 54 rows (by 53)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global returns_avg\n",
    "features2 = ['turnoverRatio',\n",
    "       'transactionAmount', 'pe_ttm', 'pe', 'pb', 'ps', 'pcf',\n",
    "       'remainder7', 'remainder14',\n",
    "       'ctc1', 'ctc5', 'ctc10',\n",
    "       'ctc20', 'daily_volume_moving_ratio_5d',\n",
    "       'daily_volume_moving_ratio_10d', 'daily_volume_moving_ratio_20d', 'SAR',\n",
    "       'SAREXT', 'RSI', 'HT_DCPERIOD', 'HT_PHASOR_inphase',\n",
    "       'HT_PHASOR_quadrature', 'HT_SINE_sine', 'HT_SINE_leadsine']\n",
    "TARGET = 'return'\n",
    "features2 = features2 + [f\"lag{i}\" for i in [700, 750, 800, 850, 900, 950]]\n",
    "FEATURES4 = ['pcf', 'pe', 'pb', 'pe_ttm', 'ps', 'transactionAmount', 'turnoverRatio', 'money', 'open', 'close', 'high', 'low'] + [f\"period{i}\" for i in range(2, 15)] + [f\"remainder{i}\" for i in range(2, 15)]\n",
    "FEATURES5 = FEATURES4 + [f\"lag{i}\" for i in [700, 750, 800, 850, 900, 950]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(df_train, df_test, asset_type, features):\n",
    "    # major changes to previous one: previous we have set arbitary where to split train and test set (e.g. 7:3)\n",
    "    # this version make uses tss to have multiple folds of train and test sets (*without information leakage)\n",
    "    lag_adjuster = df_train.shape[0]\n",
    "\n",
    "    df_train = df_train[df_train['asset'] == asset_type]\n",
    "    df_test = df_test[df_test['asset'] == asset_type]\n",
    "\n",
    "    train_map = df_train['return'].to_dict()\n",
    "    df_train = add_lag(df_train, train_map)\n",
    "    df_train['isFuture'] = False\n",
    "    df_test['isFuture'] = True\n",
    "    df_test.index += lag_adjuster\n",
    "    train_and_test = pd.concat([df_train, df_test])\n",
    "    tat_map = train_and_test['return'].to_dict()\n",
    "    train_and_test = add_lag(train_and_test, tat_map)\n",
    "    df_test = train_and_test[train_and_test['isFuture'] == True].copy()\n",
    "\n",
    "    x_all = df_train[features]\n",
    "    y_all = df_train[TARGET]\n",
    "\n",
    "    reg = xgb.XGBRegressor(n_estimators=2000,\n",
    "                    booster=\"gbtree\",\n",
    "                    objective=\"reg:linear\",\n",
    "                    max_depth=2,            #high value leads to overfitting\n",
    "                    learning_rate=0.4,\n",
    "                    min_child_weight=6,             #higher value prevent overfitting (1000:700 ratio makes it easy to overfit)\n",
    "                    subsample=1,\n",
    "                    )\n",
    "    \n",
    "    reg.fit(x_all, y_all,\n",
    "            eval_set=[(x_all, y_all)],\n",
    "            verbose=20)\n",
    "    \n",
    "    df_test['prediction'] = reg.predict(df_test[features])\n",
    "    df_progress = df_test[['prediction']]\n",
    "    #return df_progress\n",
    "    avg_adjusted_prediction = df_progress['prediction'] - (df_progress['prediction'].mean() - returns_avg[asset_type])\n",
    "    return avg_adjusted_prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_all():\n",
    "    df_train = add_features(merge_columns(df_train_market, df_train_fund, df_train_returns))\n",
    "    df_test = add_features(merge_columns(df_market, df_fund))\n",
    "    global returns_avg\n",
    "    returns_avg = [df_train[df_train['asset'] == i]['return'].mean() for i in range(54)]\n",
    "\n",
    "    df_res = train_predict(df_train, df_test, 0, features_build)\n",
    "    for i in range(1, 54):\n",
    "        df_res = pd.concat((df_res, train_predict(df_train, df_test, i, features_build)))\n",
    "\n",
    "    df_copy = df_res.sort_index()\n",
    "    df_copy = df_copy.reset_index()\n",
    "    dt_col = df_fund['date_time']\n",
    "    df_copy['date_time'] = dt_col\n",
    "    df_copy = df_copy[['date_time', 'prediction']]\n",
    "    df_copy.columns = ['date_time', 'return']\n",
    "    return df_copy\n",
    "\n",
    "df_res = train_predict_all()\n",
    "df_res\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Hacking and Hyper Parameter Tuning ðŸ‘Ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2day_return = df_market[df_market['interval'] == 50]\n",
    "df_2day_return = df_2day_return.reset_index()\n",
    "df_2day_return = df_2day_return[['date_time', 'close']]\n",
    "close_map = df_2day_return['close'].to_dict()\n",
    "\n",
    "df_2day_return['close_2day_later'] = (df_2day_return.index + 2*54).map(close_map)\n",
    "df_2day_return['2day_return'] = (df_2day_return['close_2day_later'] - df_2day_return['close']) / df_2day_return['close']\n",
    "df_2day_return = df_2day_return.fillna(0)\n",
    "real_2day_returns = df_2day_return['2day_return']\n",
    "\n",
    "df_2day_return2 = df_2day_return.copy()\n",
    "df_2day_return2['close_2day_later'] = (df_2day_return2.index - 2*54).map(close_map)\n",
    "df_2day_return2['2day_return'] = (df_2day_return2['close_2day_later'] - df_2day_return2['close']) / df_2day_return2['close']\n",
    "df_2day_return2 = df_2day_return2.fillna(0)\n",
    "real_2day_returns2 = df_2day_return2['2day_return']\n",
    "\n",
    "real_2day_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_res['return'].corr(real_2day_returns)) #should be the correct one.\n",
    "print(df_res['return'].corr(real_2day_returns2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makeshift Analysis on the return of the investment\n",
    "\n",
    "A very basic computation of the average return among the asset chosen on each day, without catering transaction fees or closing fees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = add_date(df_res)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(1001, 1002):\n",
    "    return_list = df_test[df_test['day'] == d]['return']\n",
    "    returns_based_decision = return_list.apply(lambda x: 1 if x > 0.02 else 0)\n",
    "    # returns_based_decision = list(returns_based_decision)\n",
    "    print(f\"This is Day: {d}, Returns are:\")\n",
    "    returns_based_weighting = returns_based_decision / returns_based_decision.sum()\n",
    "    print(returns_based_weighting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
